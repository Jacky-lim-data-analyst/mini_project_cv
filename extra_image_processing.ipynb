{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning outcomes\n",
    "1. white balancing techniques\n",
    "2. Edge enhancement techniques (Image sharpening)\n",
    "\n",
    "# White balancing\n",
    "\n",
    "White balancing remains a critical preprocessing step in computer vision applications, ensuring color consistency regardless of illumination conditions. OpenCV's contrib packages, particularly the xphoto module offers sophisticated algorithms for automatic white balancing. \n",
    "\n",
    "## Intro to white balance\n",
    "White balance refers to the process of removing unrealistic color casts from images, ensuring the objects appearing white in person are rendered white in captured images regardless of lighting conditions. Digital sensors, unlike the human visual system lack the adaptive capability to automatically compensate for varying illuminants. \n",
    "\n",
    "The fundamental concept behind white balancing involves estimating the color of the illuminant in a scene and then compensating for it by adjusting color channel intensities. White balancing is typically executed as a two-step process: first estimating the scene illuminant and then applying a correction to neutralize its effect on the image colors.\n",
    "\n",
    "In Python, the functionality can be accessed through the `cv2.xphoto` module. \n",
    "```\n",
    "wb = cv.createGrayWorldWB()   # for gray world algorithm\n",
    "wb = cv.createSimpleWB()   # for simple white balance\n",
    "wb = cv.createLearningBasedWB()   # for learning-based approach\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple white balance algorithm\n",
    "The SimpleWB class implements a straightforward white balance algorithm that operates by independently stretching each color channel of the input image to a specified range. This approach is based on the assumption that each color channel should utilize the full dynamic range in a properly white-balanced image.\n",
    "\n",
    "The implementation offers several configurable parameters that control its behavior:\n",
    "1. InputMin and InputMax: Defining the expected range of input pixel values.\n",
    "2. OutputMin and OutputMax: Specifying the desired range for output pixel values.\n",
    "3. P parameter: controlling the percentage of top and bottom pixel values to ignore, which increases the robustness against outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from utils import display_images\n",
    "\n",
    "# create wb class\n",
    "wb = cv.xphoto.createSimpleWB()\n",
    "\n",
    "# load image\n",
    "img = cv.imread('./images/alley_night.jpg')\n",
    "\n",
    "result = wb.balanceWhite(img)\n",
    "\n",
    "# create another wb class with different P param\n",
    "wb1 = cv.xphoto.createSimpleWB()\n",
    "wb1.setP(0.05)  # ignore 5% of top and bottom pixels\n",
    "\n",
    "result1 = wb1.balanceWhite(img)\n",
    "\n",
    "display_images([img, result, result1], \n",
    "               [\"image\", \"white balanced\", \"wb (0.05)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected input min-max of the input image:  (0.0, 255.0)\n",
      "The output ranges of the input image:  (0.0, 255.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"The expected input min-max of the input image: \", (wb.getInputMin(), wb.getInputMax()))\n",
    "print(\"The output ranges of the input image: \", (wb.getOutputMin(), wb.getOutputMax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gray world white balance algorithm\n",
    "The GrayworldWB class implements the well established gray-world assumption algorithm, which postulates that under a neutral illuminant, the average of all colors in an image should be gray. This method assumes that, in a sufficiently varied scene, the average reflectance of surfaces is achromatic (i.e., has equal RGB components).\n",
    "\n",
    "It adds a modification which thresholds pixels based on their saturation value and only uses pixels below the provided threshold in finding average pixel values.\n",
    "\n",
    "Saturation is calculated using the following for a 3-channel RGB image per pixel I and is in the range [0, 1]:\n",
    "$$Saturation[I] = \\frac{max(R,G,B) - min(R,G,B)}{max(R,G,B)}$$\n",
    "\n",
    "A threshold of 1 means that all pixels are used to white-balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gray world assumption white balancing\n",
    "wb_gray_world = cv.xphoto.createGrayworldWB()\n",
    "\n",
    "result = wb_gray_world.balanceWhite(img)\n",
    "\n",
    "display_images([img, result], (\"Image\", \"gray world white balance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_gray_world = cv.xphoto.createGrayworldWB()\n",
    "\n",
    "img = cv.imread(\"./images/night_view.jpg\")\n",
    "\n",
    "result = wb_gray_world.balanceWhite(img)\n",
    "\n",
    "display_images([img, result], (\"Image\", \"gray world white balance\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image sharpening techniques\n",
    "\n",
    "## Unsharp masking\n",
    "Unsharp masking is a linear image processing technique used to sharpen images by enhancing their perceived sharpness and clarity. In the context of digital images, unsharp masking works by subtracting a blurred version of the image from the image itself to create an \"unsharp mask\", which emphasizes edges and fine details.\n",
    "\n",
    "The math formula:\n",
    "$$V = x + \\gamma (x-y)$$\n",
    "\n",
    "Where:\n",
    "- $x$ is original input image.\n",
    "- $y$ is the blurred version of the input image, obtained using low-pass filter.\n",
    "- $\\gamma$ is a scaling factor that controls the strength of the sharpening effect.\n",
    "- $V$ is the resulting sharpened image.\n",
    "\n",
    "Alternatively, in some advanced implementations like Photoshop's Unsharp Mask, the formula involves additional steps for refinement:\n",
    "$$O_{sharpened} = O + (O - GB) - inv(O + inv(GB))$$\n",
    "\n",
    "Here:\n",
    "- $O$ is the original image\n",
    "- $GB$ represents the Gaussian-blurred version of the image.\n",
    "- $inv(.)$ denotes inversion operations. In the context of image processing, inversion typically involves flipping pixel intensity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsharp masking operation\n",
    "import numpy as np\n",
    "\n",
    "def unsharp_masking(img: np.ndarray, scaling_factor: float, ksize: int=5) -> np.ndarray:\n",
    "    \"\"\"Unsharp masking\n",
    "    Args:\n",
    "        img: (numpy array), input image\n",
    "        scaling_factor: float, sharpening effect\n",
    "        ksize: int, kernel size of blur filter\n",
    "    Returns:\n",
    "        Resulting image (numpy array)\"\"\"\n",
    "    blur = cv.blur(img, (ksize, ksize))\n",
    "    img_float = img.astype(np.float32)\n",
    "    edge_details = scaling_factor * (img_float - blur)\n",
    "    return cv.convertScaleAbs(img_float + edge_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"./images/car.jpg\")\n",
    "\n",
    "enhanced_img = unsharp_masking(img, scaling_factor=1.0)\n",
    "\n",
    "display_images([img, enhanced_img], (\"image\", \"unsharp masking\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse\n",
    "def inv(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Inversion of image: 255 - x\"\"\"\n",
    "    return 255 - img\n",
    "\n",
    "# advanced unsharp masking\n",
    "def unsharp_masking_v2(img: np.ndarray, ksize: int=5):\n",
    "    \"\"\"Advanced unsharp masking\n",
    "    Args:\n",
    "        img: numpy array, input image\n",
    "        ksize: int, default Gaussian kernel size=5\n",
    "    Returns:\n",
    "        output image (numpy array)\"\"\"\n",
    "    blurred = cv.GaussianBlur(img, (ksize, ksize), 0)\n",
    "\n",
    "    img_float = img.astype(np.float32)\n",
    "    term3 = inv(img_float + inv(blurred))\n",
    "    term2 = img_float - blurred\n",
    "    sharpened = img_float + term2 - term3\n",
    "    return cv.convertScaleAbs(sharpened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"./images/night_view.jpg\")\n",
    "\n",
    "enhanced = unsharp_masking_v2(img)\n",
    "\n",
    "display_images([img, enhanced], (\"original\", \"enhanced\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image sharpening using Laplacian\n",
    "Steps:\n",
    "1. Apply the Laplacian filter: Convolve the original image with the Laplacian kernel to produce a filtered image that emphasizes edges.\n",
    "2. Subtract the Laplacian from the original image: Subtract the Laplacian-filtered image from the original image. This step enhances edges by amplifying the differences in intensity:\n",
    "$$g(x, y) = f(x,y) - \\nabla^2f(x,y)$$\n",
    "Here, $g(x,y)$ is the sharpened image, and $f(x, y)$ is the original image.\n",
    "3. Adjust for negative values: Since the Laplacian operator can produce negative values, the result is often scaled or adjusted to ensure all pixel values remain within the valid range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sharpening using Laplacian\n",
    "def sharpening_Laplacian(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Image sharpening using Laplacian filter\n",
    "    Args:\n",
    "        img: numpy array, source\n",
    "    Returns:\n",
    "        numpy array, destination image\"\"\"\n",
    "    # convert the image to grayscale\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # apply Laplacian filter\n",
    "    laplacian = cv.Laplacian(gray, cv.CV_64F)\n",
    "    laplacian_abs = cv.convertScaleAbs(laplacian)\n",
    "\n",
    "    # combine the original gray image and the Laplacian\n",
    "    enhanced = cv.addWeighted(gray, 1.0, laplacian_abs, 1.0, 0)\n",
    "    return enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"./images/meal.jpg\")\n",
    "\n",
    "enhanced = sharpening_Laplacian(img)\n",
    "\n",
    "display_images([img, enhanced], (\"original image\", \"Laplacian filter\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge enhancement in color image\n",
    "1. Convert to a suitable color space (optional): While you can work directly in RGB, converting the image to a color space like Lab (L for lightness, a and b for color components) can be advantageous.\n",
    "2. Detect edges using a color edge detection technique: Instead of converting to grayscale and losing color-specific edge info, use a method that accounts for all channels. A powerful approach is the Di Zenzo structure tensor method, which computes the gradient across all color channels:\n",
    "    * For each channel (R, G, B), calculate the spatial gradients (e.g., using Sobel operators) in the x and y directions: $G_{xR}, G_{yR}, G_{xG}, G_{yG}, G_{xB}, G_{yB}$.\n",
    "    * Form the structure tensor by summing the outer products of these gradients over the channels.\n",
    "    * Compute the eigenvalues of the tensor; the largest eigenvalue at each pixel represents the edge strength, giving a single edge map $E$ that captures both intensity and color edges.\n",
    "3. Extract the detail layer: To enhance edges, isolate the high-frequency components (details, including edges) of the image:\n",
    "    * For each channel $c(R, G, B)$, apply a Gaussian blur to create a smoothed version $c_{blurred}$.\n",
    "    * Compute the detail layer for each channel: $D_c = c - c_{blurred}$. This represents the edges and fine details.\n",
    "4. Enhance image using the edge map: use the edge map, $E$ to guide the sharpening process:\n",
    "    * For each channel, compute the sharpened version: $c_{sharpened} = c + \\lambda.E.D_c$, where $\\lambda$ is a scaling factor to control the enhancement strength.\n",
    "    * The edge map E ensures that sharpening is stronger where edges are prominent and weaker in smooth areas, preventing over-enhancement of noise or float regions.\n",
    "5. Combine the channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_edges_color(img: np.ndarray, sigma: float=2.0, lambda_=1.0):\n",
    "    \"\"\"Enhance edges in a color image using structure tensor and detail layer amplication.\n",
    "    \n",
    "    Args:\n",
    "        - img: Input color image (BGR)\n",
    "        - sigma: standard deviation for gaussian blur (default: 2.0)\n",
    "        - lambda_: enhancement strength factor (default: 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        - enhanced_image: edge enhanced color image\"\"\"\n",
    "    # split the image into B, G, R channels\n",
    "    B, G, R = cv.split(img)\n",
    "    B = B.astype(np.float32)\n",
    "    G = G.astype(np.float32)\n",
    "    R = R.astype(np.float32)\n",
    "\n",
    "    # compute Sobel gradients for each channels (x and y directions)\n",
    "    G_xB = cv.Sobel(B, cv.CV_32F, 1, 0, ksize=3)\n",
    "    G_yB = cv.Sobel(B, cv.CV_32F, 0, 1, ksize=3)\n",
    "    G_xG = cv.Sobel(G, cv.CV_32F, 1, 0, ksize=3)\n",
    "    G_yG = cv.Sobel(G, cv.CV_32F, 0, 1, ksize=3)\n",
    "    G_xR = cv.Sobel(R, cv.CV_32F, 1, 0, ksize=3)\n",
    "    G_yR = cv.Sobel(R, cv.CV_32F, 0, 1, ksize=3)\n",
    "\n",
    "    # compute structure tensor components\n",
    "    J_11 = G_xB ** 2 + G_xG ** 2 + G_xR ** 2\n",
    "    J_12 = G_xB * G_yB + G_xG * G_yG + G_xR * G_yR\n",
    "    J_22 = G_yB ** 2 + G_yG ** 2 + G_yR ** 2\n",
    "\n",
    "    # compute the largest eigenvalue\n",
    "    discriminant = (J_11 - J_22) ** 2 + 4 * J_12 ** 2\n",
    "    lambda_max = 0.5 * (J_11 + J_22 + np.sqrt(np.maximum(discriminant, 0)))\n",
    "\n",
    "    # normalize the edge map\n",
    "    E_max = np.max(lambda_max)\n",
    "    if E_max > 0:\n",
    "        E = lambda_max / E_max\n",
    "    else:\n",
    "        E = lambda_max\n",
    "\n",
    "    # compute the blurred versions of each channel to extract details\n",
    "    B_blurred = cv.GaussianBlur(B, (0, 0), sigmaX=sigma)\n",
    "    G_blurred = cv.GaussianBlur(G, (0, 0), sigmaX=sigma)\n",
    "    R_blurred = cv.GaussianBlur(R, (0, 0), sigmaX=sigma)\n",
    "\n",
    "    # compute detail layers\n",
    "    D_B = B - B_blurred\n",
    "    D_G = G - G_blurred\n",
    "    D_R = R - R_blurred\n",
    "\n",
    "    # enhance each channel using edge map\n",
    "    B_sharpened = B + lambda_ * E * D_B\n",
    "    G_sharpened = G + lambda_ * E * D_G\n",
    "    R_sharpened = R + lambda_ * E * D_R\n",
    "\n",
    "    # clip values to [0, 255]\n",
    "    B_sharpened = np.clip(B_sharpened, 0, 255).astype(np.uint8)\n",
    "    G_sharpened = np.clip(G_sharpened, 0, 255).astype(np.uint8)\n",
    "    R_sharpened = np.clip(R_sharpened, 0, 255).astype(np.uint8)\n",
    "\n",
    "    enhanced_image = cv.merge([B_sharpened, G_sharpened, R_sharpened])\n",
    "\n",
    "    return enhanced_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"./images/book_page.jpg\")\n",
    "\n",
    "enhanced = enhance_edges_color(img, lambda_=2.0)\n",
    "\n",
    "display_images([img, enhanced], (\"original\", \"Enhanced\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
